{
  
    
        "post0": {
            "title": "Interpreting Machine Learning Model Predictions",
            "content": "The age of AI in the real world has been here a while now, and they&#39;ve achieved stunning advances in varied fields, breaking barriers no one thought they would. . But taking a closer look, we realize that while the era of AI is driven by building and using it, not a lot of progress has been made on understanding it&#39;s predictions. You know how to make it work, but you don&#39;t know why it works the way it does. . A model can predict the price of a stock to an impressive degree of accuracy, but it never makes it clear why it predicted $15, not $16 or $14. The decision process behind a model is essentially opaque, especially in modern data science. . Two major reasons for this blindspot has been . The massive of growth in the usage of Artificial Neural Networks(ANN), and it&#39;s associated techniques of machine learning(also known as &#39;Deep Learning&#39;)which are incredible algorithms that achieve fantastic results. Deep Learning and Neural Networks,leverage the recent rise in GPU access to train powerful and complex models (essentially huge matrices highly tuned using massive amounts of data). These models are the ones powering AI voice assistants, translators, facial recognition applications and many other cutting edge technologies. | The parameters in an ANN, which are used to for it&#39;s predictions, are not set by the designers themselves,but are learnt by it,after parsing the data. Due to their abstract nature,ANNs are essentially black-box machines,and while they make great predictions in their narrow domain of learning, their internal reasoning remains a mystery to builders and users. . The inability of most organizations to grasp the necessity of understanding how their models work, something that never comes up until the models starts behaving in eways that are counterpredictive to the organization&#39;s goals. | But as data scientist and author Cathy O&#39;Neil said &quot;models are opinions embedded in math&quot;. . So models carry with them things that the data scientists didn&#39;t intend, like biases it read from the data,unkown biases of the data scientist etc. So if a professors in a college course tends to grade men higher than women,knowingly or unkowingly, the model will learn that bias and predict men as being more likely to succeed in that course. . As models go rogue and cause massive losses to companies for inexplicable reasons, or make unethical judgements/predictions that disproportionately hurt disadvantaged people, the need to understand why a model makes the prediction it does is rising rapidly across the world. This has to led to researchers and problem solvers across the world to come up with techniques that allow the builders and users of AI to make more of sense of what is going in the model, that makes it behave the way it does. These ideas together form the area of model explainabilty, the field that tries to understand how the parameters in a Neural Network impact it&#39;s eventual prediction. . In this, and the next few blog posts, we will look into some of these techniques. Understanding this blog just requires a basic idea of how AI predicts things. Blogs following this one will get progressively more technical, for those who are interested in getting into the weeds. . Ok, let&#39;s get to it. . As an aside, there is a concept called model interpretability in data science. Interpretability has to do with how accurate a machine learning model can associate a cause to an effect. Explainability has to do with the ability of the parameters, often hidden in Deep Nets, to justify the results. A comprehensive dive into these concepts, their contrasts and overlaps can be found here . Not all models are created equal. . Some models, like Random Forests and Linear Regression, are easily interpretable models, and the decisions the models make when they see a piece of data can be understood with little effort. These are called glassbox models, models that we can look into to understand what&#39;s happening inside. . That isn&#39;t the case with Neural Networks though. These are called blackbox models,since we can&#39;t really understand what&#39;s going on inside it. clever methods have been introduced to go around them and understand them though. Let&#39;s start with the first one. . Model Agnostic methods . Surrogate models . A Surrogate model is an interpretable model that is trained to approximate the predictions of a black box model. We can draw conclusions about the black box model by interpreting the surrogate model. . So for example, we train a linear regression/random forest model to act as a surrogate model for a neural network that predicts housing prices, the target for the random forest isn&#39;t the actual housing prices. Instead it is the predicted output of the neural network. So the job of the surrogate model is not to be good on real world data, only to be good in mimicking/replicating the output from the neural network. (consequently, it is the Neural Network&#39;s job to actually work in the real world) . We link the output of the surrogate model and the blackbox model via some statistical measure like R^2. . Some observations. . This is an intuitive and simple workaround. | You have to be aware that you draw conclusions about the model and not about the data, since the surrogate model never sees the real outcome. | It is not clear what the best cut-off for R-squared is in order to be confident that the surrogate model is close enough to the black box model. Is 95% good enough ? for the stock market? for disease spread modeling? These are vague choices with no good answers. | The whole approach is unsatifactory in a way, since you actually have no idea how the black box model is making predictions, we have to assume (rightly?) that it somehow mimicking the same decision patterns that the surrogate model is making. | Nevertheless, we persevere. . While training the surrogate model, If we weight the data locally by a specific instance of the data (the closer the instances to the selected instance of interest, the higher their weight), we get a local surrogate model that can explain the individual prediction of the instance. This is the concept behind LIME. . LIME? . LIME : Local interpretable model-agnostic explanation . Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions. It works something like this. . Take the input features and the prediction produced by a blackbox model you want to understand | perturb the datapoint(create a bunch of datapoints from the original one by varying it&#39;s features a little bit) to get a new mini dataset. | Weight each new datapoint by how close it is to the original one, by some statistical measure | Train the surrogate model on this new set of weighted data | Intepret the surrogate model | OK. Let&#39;s check out some code. We&#39;re going to use a pre-trained neural network that predicts housing prices on the infamous Boston Housing Dataset. . import warnings warnings.filterwarnings(&#39;ignore&#39;) . import pandas as pd df = pd.read_csv(&#39;data/boston.csv&#39;) #read the data in, have a look df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT target . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . A brief overview of what each feauture means can be found here . Now let&#39;s pick a row of data,pass it through the model to get a prediction and then pass it through the OSS Lime exaplainer, which will then use a surrogate linear model to explain how each feature effected the prediction. . example = df.iloc[122,:-1] example . CRIM 0.09299 ZN 0.00000 INDUS 25.65000 CHAS 0.00000 NOX 0.58100 RM 5.96100 AGE 92.90000 DIS 2.08690 RAD 2.00000 TAX 188.00000 PTRATIO 19.10000 B 378.09000 LSTAT 17.93000 Name: 122, dtype: float64 . Load up the pre-trained Keras Model . from tensorflow.keras.models import load_model model = load_model(&#39;models/boston_keras_reg.tf&#39;) . prediction = model.predict(example.values.reshape(1, -1)) #prediction prediction . array([[14.154355]], dtype=float32) . from lime.lime_tabular import LimeTabularExplainer explainer = LimeTabularExplainer(df.drop(&#39;target&#39;,axis=1).values, mode=&quot;regression&quot;, feature_names=df.columns.tolist()[:-1], discretize_continuous=False) . The parameters passed to the explainer are: . our training set, we need to make sure we use the training set without one hot encoding | mode: the explainer can be used for classification or regression | feature_names: list of labels for our features | categorical_features: list of indexes of categorical features | categorical_names: dict mapping each index of categorical feature to a list of corresponding labels | dicretize_continuous: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature. | . explanation = explainer.explain_instance(example,model.predict) . explanation.show_in_notebook() . . . The images above and the coefficients (of the linear model that was trained on the dataset created by the perturbed datapoint), convey how the features values of the data point influenced the prediction. . So this is essentially, global feature importance, except that since the data is close to the actual original datapoint and the datapoints are weighted by how close they are to the original datapoint, the model hopefully tells how each feature value influenced this particular prediction. . Now, on to shapely values . Shapley Values : A Game theoretic concept applied to data . Named after Lloyd Shapley, the 2012 recipient of the Nobel Prize in economics, Shapley values are an extremely useful concept. WikiPedia, on Shapley values, says : To each cooperative game it(shapley values) assigns a unique distribution (among the players) of a total surplus generated by the coalition of all players. . An idea like this can be used, for example, to divide profits(&quot;surplus&quot;) among a bunch of people, based on their contribution. . While the concept seems foreign to Machine Learning, and every word in the definition as actually more technical/mathematical than it sounds, for our purposes, it suffices to say that a machine learning model that uses features to make predictions can be a safely called a cooperative game, with each feature playing the role of a player, and the prediction being the total surplus generated by the players/features. . But a surplus implies an initial value. In the case of finance, the initial value would be the investment, and the profit made would be the surplus. . But what would initial value and surplus be, in machine learning? . If you have a dataset that has a bunch of features(say size, location,number of rooms etc) that predict the price of houses, and you are asked, without supplying any data, to guess the price of a random house,What would you do? The most sensible answer in a scenario like that, would be just give the average house price(the average target value in the training data). . That, right there, is your initial value. So you start with the average housing price (say $30,000),provide the model with features and their values, and after the model spits out a prediction (say $42,000),the shapely algorithm allots each feature a number(positive/negative) based on their contribution in getting the housing price from $32,000 to get to your predicted value of $42,000. . So if the size of a house was bigger than average in a pirticular data point, and contributed to increasing the price of a house from $30,000 (to $32,5000, let&#39;s say), then &quot;size&quot; would be alloted a positive shapley value, and so on. But notice this isn&#39;t very straight forward. If the size of the house was extremely big(, and the model is well trained), then it might very well decrease the value of the house from the average price, leading to it being assigned a negative shapley value. . To look at shapley values in action, we will be looking at the OSS shap library and it&#39;s implementation of KERNEL shap.The Kernel SHAP (SHapley Additive exPlanations) algorithm is based on the paper A Unified Approach to Interpreting Model Predictions by Lundberg et al. For our purposes, it does the job. For a simple and quick comparison, read here . import shap import numpy as np #the explainer expects the predict function of the model, and a sample of the data to calculate the expected value explainer = shap.KernelExplainer(model.predict, df.drop(&#39;target&#39;,axis=1)) shap_values = np.array(explainer.shap_values(example)) print(&quot;shap_values:&quot;,shap_values) expected_value = explainer.expected_value print(&quot;expected_value:&quot;,expected_value) . Using 506 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples. . shap_values: [[-6.97313932e-02 -7.40173258e-01 -2.72785574e+00 -5.37045645e-02 -9.85263754e-04 -1.59483442e-01 1.17676905e+00 8.57806576e-01 -3.45635606e-01 4.66582391e+00 -1.81441254e-01 2.10089121e-01 -4.05261644e+00]] expected_value: [15.57549335] . The numbers above, quite literally, show their contribution to influencing the model, into predicting the output that it did. The shapley values and the expected value, sum up to the model prediction . print(&quot;sum of the expected value and the shap values&quot;,np.sum(shap_values) + expected_value) print(&quot;prediction by model&quot;,prediction.tolist()[0]) np.sum(shap_values) + expected_value == prediction . sum of the expected value and the shap values [14.15435505] prediction by model [14.1543550491333] . array([[ True]]) . The most enlightening way to see this is using waterfall charts. . import plotly.graph_objects as go from IPython.display import HTML fig = go.Figure(go.Waterfall( name = &quot;Feature contribution&quot;, orientation = &quot;v&quot;, measure = ([&quot;initial&quot;] + [&quot;relative&quot;]*(len(df.columns[:-1])) + [&quot;total&quot;]), x = [&quot;Expected Value&quot;]+list(df.columns[:-1])+[&quot;Predicted Value&quot;] , textposition = &quot;outside&quot;, y = [expected_value[0]] + shap_values[0].tolist() + [prediction], connector = {&quot;line&quot;:{&quot;color&quot;:&quot;rgb(63, 63, 63)&quot;}}, )) fig.update_layout( title = &quot;How the model predicted the price of a house&quot;, showlegend = True ) fig.update_yaxes(range=[10, 20]) fig.show() . from IPython.display import Image Image(filename=&#39;images/fig1.png&#39;) . This chart shows, very intuitively how each feature influences the final output. We know that the LSTAT feature of the house in question was 17.9, and significantly pushed the price of the house down. . Let&#39;s look at a different datapoint, with a lower LSTAT value to see how it does. . new_example = df.iloc[281,:-1] new_example . CRIM 0.03705 ZN 20.00000 INDUS 3.33000 CHAS 0.00000 NOX 0.44290 RM 6.96800 AGE 37.20000 DIS 5.24470 RAD 5.00000 TAX 216.00000 PTRATIO 14.90000 B 392.23000 LSTAT 4.59000 Name: 281, dtype: float64 . prediction = model.predict(new_example.values.reshape(1, -1)) prediction . array([[25.612377]], dtype=float32) . explainer = shap.KernelExplainer(model.predict, df.drop(&#39;target&#39;,axis=1)) shap_values = np.array(explainer.shap_values(new_example)) print(&quot;shap_values:&quot;,shap_values) expected_value = explainer.expected_value print(&quot;expected_value:&quot;,expected_value) . Using 506 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples. . shap_values: [[-0.12505208 -0.39688981 1.43853405 -0.05990218 -0.01562481 0.28294905 -1.71356051 -0.60695652 -0.25592982 3.18804717 0.85869149 1.71991603 5.72266175]] expected_value: [15.57549335] . fig = go.Figure(go.Waterfall( name = &quot;Feature contribution&quot;, orientation = &quot;v&quot;, measure = ([&quot;initial&quot;] + [&quot;relative&quot;]*(len(df.columns[:-1])) + [&quot;total&quot;]), x = [&quot;Expected Value&quot;]+list(df.columns[:-1])+[&quot;Predicted Value&quot;] , textposition = &quot;outside&quot;, y = [expected_value[0]] + shap_values[0].tolist() + [prediction], connector = {&quot;line&quot;:{&quot;color&quot;:&quot;rgb(63, 63, 63)&quot;}}, )) fig.update_layout( title = &quot;How the model predicted the price of a house&quot;, showlegend = True ) fig.update_yaxes(range=[10, 28]) . Image(filename=&#39;images/fig2.png&#39;) . In this case, a higher LSTAT contributed positively to the final value. This opens more doors to understanding, not only your model&#39;s behavior, but also interesting patterns in the data, that were not visible before! . Concluding thoughts . On the whole, shapley values are preferred explainer than LIME mostly because, LIME models are surrogate models, proxies for the model in question, with a very tenous connection between the two. Shapley values on the other hand directly interact with the model giving more dependable results. . Shapley values aren&#39;t perfect though. There are: . academic criticisms of it (including the fact that it&#39;s most efficient implementation has exponential runtime wrt the number of features it has to explain) | improvements on it | Alternative methods of explanation, like those offered by Eli5 | Limitations to it&#39;s applications, including the inability to apply it directly to problems involving image classification and localization. There are variations to SHAP and alternatives like CAMs and integrated gradients that are available | A deeper, more technical understanding of Shapley values will require a gentle introduction to Game theory and statistics,all of which will be done in upcoming posts. . To be continued... . Further Reading: . Christopher Molnar&#39;s open source book on Interpretable machine learning | A comprehensive Deep dive into shapley values, a great step up from the information here, can be read here | At Censius, our team of expert data scientists and engineers are building an observability platform to enable visibility into models at scale for businesses. Learn more about us here .",
            "url": "https://vishakbharadwaj94.github.io/Neural_Breakdown/2020/03/07/explanations.html",
            "relUrl": "/2020/03/07/explanations.html",
            "date": " • Mar 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My Name is Vishak Bharadwaj,I’m a Machine Learning Engineer at [Censius.ai]. I’ve been an ML engineer for a couple of years now, I’ve had a wild journey into the field. A lot of people helped me on the journey, including the people who made this website. This blog is my way of giving back! . My Blogposts will be deepdives into various aspects of deep learning, from data prep to deployment to everything in between. Here’s hoping it provides all of you much value! .",
          "url": "https://vishakbharadwaj94.github.io/Neural_Breakdown/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vishakbharadwaj94.github.io/Neural_Breakdown/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}